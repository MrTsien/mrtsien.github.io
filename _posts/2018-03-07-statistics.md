---
layout:     post
title:      "统计学习方法"
subtitle:   "笔记"
date:       2018-03-07 14:03:58
author:     "MrTsien"
header-img: "img/post-bg-unix-linux.jpg"
catalog: true
tags:
    - BigData
---
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

# 统计学习方法概论

## 统计学习

+ 特点
    1. 统计学习以计算机及网络为平台，是简历在计算机及网络之上的；
    2. 统计学习以数据为研究对象，是数据驱动的学科；
    3. 统计学习的目的地是对数据进行预测与分析；
    4. 统计学习以方法为中心，统计学习方法构建模型并应用模型进行预测与分析；
    5. 统计学习是概率论、统计学、信息论、计算理论、最优化理论及计算机科学等多个领域的交叉学科，并且在发展中逐步形成独自的理论体系与方法论

+ 对象
    - 数据

+ 目的
    - 对数据进行预测与分析

+ 方法
    - 方法是基于数据构建统计模型从而对数据进行预测与分析
        1. 监督学习（supervised learning）
        2. 非监督学习（unsupervised learning）
        3. 半监督学习（semi-supervised learning）
        4. 强化学习（reinforcementlearning）
    - 统计学三要素
        1. 模型（model）
        2. 策略（strategy）
        3. 算法（algorithm）

## 监督学习

+ 基本概念
    1. 输入空间、特征空间与输出空间
        + 每个具体的输入是一个实例（instance），通常有特征向量（feature vector）表示。
        + 所有特征向量存在的空间称为特征空间（feature vector）。
        + 输入变量X和输出变量Y有不同的类型，可以是连续的，也可以是离散的。
            - 输入变量与输出变量均为连续变量的预测问题称为回归问题。
            - 输出变量为有限个离散比那辆的预测问题称为分类问题。
            - 输入变量与输出变量均为变量序列的预测问题称为标注问题。

    2. 联合概率分布
        + 监督学习假设输入与输出的随机变量X和Y遵循联合概率分布P(X,Y)。P(X,Y)表示分布函数，或分布密度函数。

    3. 假设空间
        + 监督学习的目的在于学习一个由输入到输出的映射，这一映射由模型来表示。
        + 模型属于由输入空间到输出空间的映射的集合，这个几个就是假设空间（hypothesis space）。
        + 监督学习的模型可以说概率模型或非概率模型，由条件概率分布P(Y\|X) 或决策函数（decision function）Y = f(X)表示，随具体学习方法而定。
        + 对具体的输入进行相应的输出预测时，写作 P(y\|x) 或 y = f(x)

+ 问题的形式化
    - 监督学习分为学习和预测两个过程

## 统计学习三要素

+ 统计方法 = 模型+策略+算法
	1. 模型
        + 模型就是所要学习的条件概率分布或决策函数。
        + 模型的假设空间（hypothesis space）包含所有可能的条件概率分布或决策函数。（假设空间中的模型一般有无穷多个）

	2. 策略
		+ 统计学习的目：在于从假设空间中选取最优模型。
        + 损失函数和风险函数
            + 监督学习问题是在假设空间F中选取模型f作为决策函数，对于给定的输入X，由f(X)给出相应的输出Y，这个输出的预测值f(X)与真实值Y可能一致也可能不一致，用一个损失函数（loss function）或代价函数（costfunction）来度量预测错误的程度。损失函数是f(X)和Y的非负实值函数，计作L(Y,f(X))。
            + 统计学习常用的损失函数有以下几种。
                1. 0-1损失函数（0-1 loss function）
                    - $$L(Y,f(X))=\begin{cases} 1, Y \neq f(X)\\ 0, Y = f(X) \end{cases}$$
                2. 平方损失函数（quadratic loss function）
                    - $$L(Y,f(X))=(Y-f(X))^2$$
                3. 绝对损失函数（absolute loss function）
                    - $$L(Y,f(X))=|Y-f(X)|$$
                4. 对数损失函数（logarithmic loss function）或对数似然损失函数（log-likelihood loss function）
                    - $$L(Y,P(Y|X))=-logP(Y|X)$$
            + 损失函数值越小，模型就越好。由于模型的输入、输出（X,Y）是随机变量，遵循联合分布P(X,Y),所以损失函数的期望是
                - $$R_{exp}(f)=E_p[L(Y,f(X))]=\int_{x*y}L(y,f(x))P(x,y)dxdy$$
                - 这是理论上模型f(x)关于联合分布P(X,Y)的平均意义下的损失，称为风险函数（risk function）或期望损失（expected loss）。
        + 经验风险最小化与结构风险最小化
            + 经验风险最小的模型是最优的模型
                - $$ \min\limits_{f \in F} \frac {1}{N} \sum_{i=1}^{N} L(y_{i},f(x_{i}))$$
                - 极大似然估计（maximum likelihood estimation）就是经验风险最小化的一个例子。当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。（当样本容量很小时，会产生过拟合（over-fitting）现象）
                - 结构风险最小化（structural risk minimization，SRM）是为了防止过拟合二提出来的策略。结构风险最小化等价于正则化（regularizer）或罚项（penalty term）。
                    - $$ R_{srm}(f) = \frac {1}{N} \sum_{i=1}^{N} L(y_{i},f(x_{i}))+\lambda J(f)$$
                - 结构风险最小化的策略认为结构风险最小的模型是最优的模型，所以求最优模型，就是求解最优化问题：
                    - $$ \min\limits_{f \in F} \frac {1}{N} \sum_{i=1}^{N} L(y_{i},f(x_{i}))+\lambda J(f)$$
    
    3. 算法
        + 算法是指学习模型的具体计算方法。统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么样的计算方法求解最优模型。

## 模型评估与模型选择

+ 训练误差与测试误差
    - 训练误差的大小，对判断给定的问题是不是一个容易学习的问题是有意义的，但本质上不重要。
    - 测试误差反映了学习方法对未知的测试数据集的预测能力，是学习中的重要盖帘。显然，给定两种学习方法，测试误差晓得方法具有更好的预测能力，是更有效的方法。
    - 通常讲学习方法对未知数据的预测能力成为泛化能力（generalization ability）。

+ 过拟合与模型选择
    - 过拟合是指学习时选择的模型所包含的参数过多，以致于出现这一模型对已知数据预测得很好，但对未知数据预测得很差的现象。
    - 可以说模型选择旨在避免过拟合并提高模型的预测能力。

## 正则化与交叉验证

+ 正则化
    - 模型选择的典型方法是正则化（regularization）。
    - 正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项（regularizer）或罚项（penalty term）。
+ 交叉验证
    - 如果给定的样本数据充足，进行模型选择的一种简单方法是随机地将数据集切分为三部分，分别为训练集（training set）、验证集（validation set）和测试集（test set）。但实际应用中数据是不充足的，为了选择好的模型，可以采用交叉验证方法。交叉验证的基本想法是重复地使用数据；把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选择。
    - 交叉验证分类
        1. 简单交叉验证
        2. S折交叉验证
        3. 留一交叉验证

## 泛化能力

+ 泛化误差
    - 学习方法的泛化能力（generalization ability）是指由该方法学习到的模型对未知数据的预测能力，是学习方法本质上重要的性质。
    - 如果学到的模型是$f^{^}$,那么用这个模型对未知数据预测的误差即为泛化误差（generalization error）
        - $$R_{exp}(f) = E_{p}[L(Y,f(X))] = \int_{x*y}L(y,f)P(x,y)dxdy$$
    - 事实上，泛化误差就是所学习到的模型的期望风险。

## 生成模型与判别模型

+ 监督学习的任务就是学习一个模型，应用这一模型，对给定的输入预测相应的输出。这个模型的一般形式为决策函数：Y=f(X)或者条件概率分布：P(Y\|X)
+ 监督学习方法又可以分为生成方法（generative approach）和判别方法（discriminative approach）。所学到的模型分别称为生成模型（generative model）和判别模型（discriminative model）。
+ 生成方法由数据学习联合概率分布P(X,Y)，然后求出条件概率分布P(Y\|X)作为预测的模型，即生成模型：
    - $$P(Y|X)=\frac{P(X,Y)}{P(X)}$$

## 分类问题

+ 评价分类器性能的指标一般是分类准确率（accuracy），其定义是：对于给定的测试数据集，分类器正确分类的样本数与总样本数之比。也就是损失函数是0-1损失时测试数据集上的准确率。
+ 对于二类分类问题常用的评价指标是精确率（precision）与召回率（recall）。通常以关注的类为正类，其他类为负类，分类器在测试数据集上的预测或正确或不正确，4种情况出现的总数分别记作：
    - TP : 将正类预测为正类数；
    - FN : 将正类预测为负类数；
    - FP : 将负类预测为正类数；
    - TN : 将负类预测为负类数。
    
    - 精确率定义为：
        - $$ P = \frac{TP}{TP+FP} $$
    - 召回率定义为：
        - $$ R = \frac{TP}{TP+FN} $$
    - 此外，还有$F_1$值，是精确率和召回率的调和均值，即
        - $$ \frac{2}{F_1} = \frac{1}{P}+\frac{1}{R} $$
        - $$ F_1 = \frac{2TP}{2TP+FP+FN} $$
    - 精确率和诈骗汇率都高时，F1值也会高。
    - 许多统计学习方法可以用于分类，包括K近邻法、感知机、朴素贝叶斯法、决策树、决策列表、逻辑斯蒂回归模型、支持向量机、提升方法、贝叶斯网络、神经网络、Winnow等

## 标注问题

+ 标注（tagging）也是一个监督学习问题。可以认为标注问题是分类问题的一个推广，标注问题又是更复杂的结构预测（structure prediction）问题的简单形式。
+ 标注问题的输入是一个观测序列，输出是一个标记序列或状态序列。
+ 标注问题的目标在于学习一个模型。
+ 评价标注模型的指标与评价分类模型的指标一样，常用的有标注准确率、精确率和召回率。其定义与分类模型相同。
+ 标注常用的统计学习方法有：隐马尔科夫模型、条件随机场。

## 回归问题

+ 回归（regression）是监督学习的另一个重要问题。
+ 回归用于预测输入变量（自变量）和输出变量（因变量）之间的关系，特别是当输入变量的值发送变化时，输出变量的值随之发生的变化。
+ 回归模型正是表示从输入变量到输出变量之间映射的函数。
+ 回归问题的学习等价于函数拟合：选择一条函数曲线使其很好地拟合已知数据且很好地预测未知数据。

+ 按照输入变量的个数，分为一元回归和多元回归。
+ 按照输入变量和输出变量之间关系的类型即模型的类型，分为线性回归和非线性回归。


# 感知机

+ 感知机模型
    - 假设输入空间（特征空间）是 $$x \subseteq R^{n}$$，输出空间是$$y = \{+1,-1\}$$，输入$$x\in X$$ 表示实例的特征向量，对应于输入空间（特征空间）的点：输出$$y\in Y$$表示实例的类别。由输入空间到输出空间的如下函数
        - $$f(x)=sign(w·x + b)$$ 
    - 称为感知机。其中，$$w$$和$$b$$为感知机模型参数，$$w\in R^n$$叫作权值（weight）或权值向量（weight vector），$$b\in R$$叫作偏置（bias），$$w·x$$表示$$w$$和$$x$$的内积。sign是符号函数，即
        - $$sign(x)=\begin{cases} +1, x \geq 0\\ -1, x < 0 \end{cases}$$
    - 感知机是一种线性分类模型，属于判别模型。感知机模型的假设空间是定义在特征空间中的所有线性分类模型（linear classification model）或线性分类器（linear classifier），即函数集合$$\{f\|f(x)=w·x + b\}$$。
        - 感知机有如下几何解释：线性方程
            - $$w·x + b = 0$$
    - 对应于特征空间$$R^n$$中的一个超平面$$S$$，其中$$w$$是超平面的法向量，$$b$$是超平面的截距。这个超平面将特征空间划分为两个部分。位于两部分的点（特征向量）分别被分为正、负两类。因此，超平面$$S$$称为分离超平面（separating hyperplane）。
        - ![perceptron](http://www.mrtsien.com/img/posts/perceptron.png)

+ 感知机学习策略
    - 数据集的线性可分性
        - 如果存在某个超平面$$S$$
            - $$w·x + b = 0$$
        - 能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有$$y_{i} = +1$$的实例$$i$$，有$$w·x_{i} + b > 0$$，对所有$$y_{i} = -1$$的实例$$i$$，有$$w·x_{i} + b < 0$$，则称数据集$$T$$为线性可分数据集（linearly separable data set）；否则，称数据集$$T$$线性不可分。
    - 感知机学习策略
        - 假设训练数据集是线性可分的，感知机学习的目标是求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面。为了找出这样的超平面，即确定感知机模型参数$$w,b$$，需要确定一个学习车里，即定义（经验）损失函数并将损失函数极小化。
    - 感知机学习算法
        - 求参数$$w,b$$，使其为一下损失函数极小化问题的解
            - $$\min\limits_{w,b}L(w,b)=-\sum\limits_{x_i\in M}y_{i}(w·x_{i}+b)$$，其中M为误分类点的集合。
        - 算法（感知机学习算法的原始形式）
            - 输入：训练数据集$$T=\{(x_1,y_1),(x_2,y_2),···,(x_N,y_N)\}$$，其中$$x_i \in  X = R^n$$,$$y_i \in Y = \{-1,+1\},i=1,2,···,N$$；学习率$$\eta(0<\eta≤1)$$;
            - 输出：$$w,b$$；感知机模型$$f(x)=sign(w·x + b)$$。
            - (1)选取初值$$w_0,b_0$$
            - (2)在训练集中选取数据$$(x_i,y_i)$$
            - (3)如果$$y_i(w·x_i+b)≤0$$
                - $$w \leftarrow w + \eta y_ix_i$$
                - $$b \leftarrow b + \eta y_i$$
            - (4)转至(2)，直至训练集中没有误分类点。
            - 这种学习算法直观上有如下解释：当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整$$w,b$$的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，使分离超平面向该误分类点使其被正确分类。
        - 算法（感知机学习算法的对偶形式）
            - 输入：线性可分的数据集$$T=\{(x_1,y_1),(x_2,y_2),···,(x_N,y_N)\}$$，其中$$x_i \in X = R^n$$,$$y_i \in Y = \{-1,+1\},i=1,2,···,N$$；学习率$$\eta(0<\eta≤1)$$;
            - 输出：$$\alpha b$$；感知机模型$$f(x) = sign(\sum_{j=1}^N\alpha_jy_jx_j·x+b$$
            - 其中$$\alpha=(\alpha_1,\alpha_2,···,\alpha_N)^T$$
            - (1)$$\alpha \leftarrow 0,b \leftarrow 0$$
            - (2)在训练集中选取数据$$(x_i,y_i)$$
            - (3)如果$$y_i(\sum_{j=1}^N\alpha_jy_jx_j·x_i+b)≤0$$
                - $$\alpha \leftarrow \alpha_i+\eta$$
                - $$b \leftarrow b + \eta y_i$$
            - (4)转至（2）直到没有误分类数据。
            - 对偶形式中训练实例仅以内积的形式出现，为了方便，可以预先将训练集中实例间的内积计算出来并以矩阵的形式存储，这个矩阵就是所谓的Gram矩阵（Gram matrix）
                - $$G = [x_i·x_j]_{N×N}$$

# K近邻法
+ K近邻算法
    - K近邻算法简单、直观：给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类。

+ K近邻模型
    - K近邻法使用的模型实际上对应于对特征空间的划分。模型由三个基本要素------距离度量、k值的选择和分类决策规则决定。
    - 模型
        - k近邻法中，当训练集、距离度量（如欧氏距离）、k值及分类决策规则（如多数表决）确定后，对于任何一个新的输入实例，它所属的类唯一地确定。
    - 距离度量
        - 特征空间中两个实例点的距离是两个实例点相似程度的反映。k近邻模型的特征空间一般是$$n$$维实数向量空间$$R^n$$。使用的距离是欧式距离，但也可以是其他距离，如更一般的$$L_p$$距离（$$L_p$$ distance）或 Minkowski距离（Minkowski distance）。
    - K值的选择
        - k值的选择会对k近邻法的结果产生重大影响。
        - 在应用中，k值一般取一个比较小的数值。通常采用交叉验证法来选取最优的k值。
    - 分类决策规则
        - k近邻法中的分类决策规则往往是多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类。
        - 多数表决规则（majority voting rule）有如下解释：如果分类的损失函数为0-1损失函数，分类函数为
            - $$\int:R^n \rightarrow \{c_1,c_2,···,c_k\}$$
        - 那么误分类的概率是
            - $$P(Y	\neq \int(X))=1-P(Y=\int(X))$$
        - 对给定的实例$$x \in X$$，其最近邻的k个训练实例点构成集合$$N_k(x)$$。如果涵盖$$N_k(x)$$的区域的类别是$$c_j$$，那么误分类是
            - $$\frac {1}{k} \sum_{x_i \in N_k(x)} I(y_i \neq c_j)=1-\frac {1}{k} \sum_{x_i \in N_k(x)} I(y_i = c_j)$$
        - 要使分类率最小即经验风险最小，就要使$$\sum_{x_i \in N_k(x)}I(y_i = c_i)$$最大，所以多数表决规则等价于经验风险最小化。

+ k近邻法的实现：kd树
    - 构造kd树
        - kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。kd树是二叉树，表示对k维空间的一个划分（partition）。构造kd树相当于不断地用垂直于坐标轴的超平面将k维空间划分，构造一系列的k维超矩形区域。kd树的每个结点对应一个k维超矩形区域。
